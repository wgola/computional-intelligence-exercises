{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wstęp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybrany temat\n",
    "\n",
    "todo\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pobieranie danych\n",
    "\n",
    "todo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_tweets(max_tweets_per_day, date_from, date_to):\n",
    "    tweets = []\n",
    "\n",
    "    date_from_2 = date_from + datetime.timedelta(days=1)\n",
    "\n",
    "    while date_from != date_to:\n",
    "        query = f'#fifaworldcup lang:en since:{date_from} until:{date_from_2}'\n",
    "        new_tweets = []\n",
    "        for _, tweet in enumerate(sntwitter.TwitterHashtagScraper(query, maxEmptyPages=5).get_items()):\n",
    "            new_tweets.append(tweet)\n",
    "\n",
    "            if len(new_tweets) >= max_tweets_per_day:\n",
    "                break\n",
    "\n",
    "        if len(new_tweets) < max_tweets_per_day:\n",
    "            query = f'#worldcup lang:en since:{date_from} until:{date_from_2}'\n",
    "            for _, tweet in enumerate(sntwitter.TwitterHashtagScraper(query, maxEmptyPages=5).get_items()):\n",
    "                new_tweets.append(tweet)\n",
    "\n",
    "                if len(new_tweets) >= max_tweets_per_day:\n",
    "                    break\n",
    "\n",
    "        if len(new_tweets) < max_tweets_per_day:\n",
    "            query = f'#wc lang:en since:{date_from} until:{date_from_2}'\n",
    "            for _, tweet in enumerate(sntwitter.TwitterHashtagScraper(query, maxEmptyPages=5).get_items()):\n",
    "                new_tweets.append(tweet)\n",
    "\n",
    "                if len(new_tweets) >= max_tweets_per_day:\n",
    "                    break\n",
    "\n",
    "        tweets.extend(new_tweets)\n",
    "        date_from += datetime.timedelta(days=1)\n",
    "        date_from_2 += datetime.timedelta(days=1)\n",
    "\n",
    "    df = pd.DataFrame(tweets)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 52927\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"data_wc.csv\", sep=\"\\t\")\n",
    "\n",
    "print(f'Number of tweets: {len(tweets)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przetwarzanie zebranych danych\n",
    "\n",
    "## Wyodrębnienie potrzebnych danych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[[\"date\", \"renderedContent\"]]\n",
    "\n",
    "tweets[\"date\"] = tweets.apply(\n",
    "    lambda x: datetime.datetime.strptime(x[\"date\"], \"%Y-%m-%d %H:%M:%S+00:00\").date(), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacja tweetów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweets[\"tokenizedContent\"] = tweets.apply(\n",
    "    lambda x: word_tokenize(x[\"renderedContent\"]), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usunięcie stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "tweets[\"tokensWithoutStopwords\"] = tweets.apply(lambda x: [\n",
    "    word for word in x[\"tokenizedContent\"] if word not in english_stopwords], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematyzacja\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tweets[\"lemmatizedContent\"] = tweets.apply(lambda x: [lemmatizer.lemmatize(\n",
    "    word) for word in x[\"tokensWithoutStopwords\"]], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyświetlenie bazy tweetów po przygotowaniu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>tokenizedContent</th>\n",
       "      <th>tokensWithoutStopwords</th>\n",
       "      <th>lemmatizedContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>FIFA World Cup Trophy arrives in Tehran dlvr.i...</td>\n",
       "      <td>[FIFA, World, Cup, Trophy, arrives, in, Tehran...</td>\n",
       "      <td>[FIFA, World, Cup, Trophy, arrives, Tehran, dl...</td>\n",
       "      <td>[FIFA, World, Cup, Trophy, arrives, Tehran, dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>Iran puts FIFA World Cup trophy on display for...</td>\n",
       "      <td>[Iran, puts, FIFA, World, Cup, trophy, on, dis...</td>\n",
       "      <td>[Iran, puts, FIFA, World, Cup, trophy, display...</td>\n",
       "      <td>[Iran, put, FIFA, World, Cup, trophy, display,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>WATCH: Japanese referee Yoshimi Yamashita is s...</td>\n",
       "      <td>[WATCH, :, Japanese, referee, Yoshimi, Yamashi...</td>\n",
       "      <td>[WATCH, :, Japanese, referee, Yoshimi, Yamashi...</td>\n",
       "      <td>[WATCH, :, Japanese, referee, Yoshimi, Yamashi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>Japanese referee Yoshimi Yamashita is set to m...</td>\n",
       "      <td>[Japanese, referee, Yoshimi, Yamashita, is, se...</td>\n",
       "      <td>[Japanese, referee, Yoshimi, Yamashita, set, m...</td>\n",
       "      <td>[Japanese, referee, Yoshimi, Yamashita, set, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>Japanese referee Yoshimi Yamashita is set to m...</td>\n",
       "      <td>[Japanese, referee, Yoshimi, Yamashita, is, se...</td>\n",
       "      <td>[Japanese, referee, Yoshimi, Yamashita, set, m...</td>\n",
       "      <td>[Japanese, referee, Yoshimi, Yamashita, set, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52922</th>\n",
       "      <td>2023-01-30</td>\n",
       "      <td>@RheaRipley_WWE First Aussie to win the #Royal...</td>\n",
       "      <td>[@, RheaRipley_WWE, First, Aussie, to, win, th...</td>\n",
       "      <td>[@, RheaRipley_WWE, First, Aussie, win, #, Roy...</td>\n",
       "      <td>[@, RheaRipley_WWE, First, Aussie, win, #, Roy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52923</th>\n",
       "      <td>2023-01-30</td>\n",
       "      <td>Check out Christian Pulisic Fifa World Cup 202...</td>\n",
       "      <td>[Check, out, Christian, Pulisic, Fifa, World, ...</td>\n",
       "      <td>[Check, Christian, Pulisic, Fifa, World, Cup, ...</td>\n",
       "      <td>[Check, Christian, Pulisic, Fifa, World, Cup, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52924</th>\n",
       "      <td>2023-01-30</td>\n",
       "      <td>Check out Gabriel Jesus Brazil 2021-22 Panini ...</td>\n",
       "      <td>[Check, out, Gabriel, Jesus, Brazil, 2021-22, ...</td>\n",
       "      <td>[Check, Gabriel, Jesus, Brazil, 2021-22, Panin...</td>\n",
       "      <td>[Check, Gabriel, Jesus, Brazil, 2021-22, Panin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52925</th>\n",
       "      <td>2023-01-30</td>\n",
       "      <td>Check out Erling Haaland Panini Donruss 2022 F...</td>\n",
       "      <td>[Check, out, Erling, Haaland, Panini, Donruss,...</td>\n",
       "      <td>[Check, Erling, Haaland, Panini, Donruss, 2022...</td>\n",
       "      <td>[Check, Erling, Haaland, Panini, Donruss, 2022...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52926</th>\n",
       "      <td>2023-01-30</td>\n",
       "      <td>@scparametro Welcome to fifa world cup u20 Ind...</td>\n",
       "      <td>[@, scparametro, Welcome, to, fifa, world, cup...</td>\n",
       "      <td>[@, scparametro, Welcome, fifa, world, cup, u2...</td>\n",
       "      <td>[@, scparametro, Welcome, fifa, world, cup, u2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52927 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date                                    renderedContent   \n",
       "0      2022-09-01  FIFA World Cup Trophy arrives in Tehran dlvr.i...  \\\n",
       "1      2022-09-01  Iran puts FIFA World Cup trophy on display for...   \n",
       "2      2022-09-01  WATCH: Japanese referee Yoshimi Yamashita is s...   \n",
       "3      2022-09-01  Japanese referee Yoshimi Yamashita is set to m...   \n",
       "4      2022-09-01  Japanese referee Yoshimi Yamashita is set to m...   \n",
       "...           ...                                                ...   \n",
       "52922  2023-01-30  @RheaRipley_WWE First Aussie to win the #Royal...   \n",
       "52923  2023-01-30  Check out Christian Pulisic Fifa World Cup 202...   \n",
       "52924  2023-01-30  Check out Gabriel Jesus Brazil 2021-22 Panini ...   \n",
       "52925  2023-01-30  Check out Erling Haaland Panini Donruss 2022 F...   \n",
       "52926  2023-01-30  @scparametro Welcome to fifa world cup u20 Ind...   \n",
       "\n",
       "                                        tokenizedContent   \n",
       "0      [FIFA, World, Cup, Trophy, arrives, in, Tehran...  \\\n",
       "1      [Iran, puts, FIFA, World, Cup, trophy, on, dis...   \n",
       "2      [WATCH, :, Japanese, referee, Yoshimi, Yamashi...   \n",
       "3      [Japanese, referee, Yoshimi, Yamashita, is, se...   \n",
       "4      [Japanese, referee, Yoshimi, Yamashita, is, se...   \n",
       "...                                                  ...   \n",
       "52922  [@, RheaRipley_WWE, First, Aussie, to, win, th...   \n",
       "52923  [Check, out, Christian, Pulisic, Fifa, World, ...   \n",
       "52924  [Check, out, Gabriel, Jesus, Brazil, 2021-22, ...   \n",
       "52925  [Check, out, Erling, Haaland, Panini, Donruss,...   \n",
       "52926  [@, scparametro, Welcome, to, fifa, world, cup...   \n",
       "\n",
       "                                  tokensWithoutStopwords   \n",
       "0      [FIFA, World, Cup, Trophy, arrives, Tehran, dl...  \\\n",
       "1      [Iran, puts, FIFA, World, Cup, trophy, display...   \n",
       "2      [WATCH, :, Japanese, referee, Yoshimi, Yamashi...   \n",
       "3      [Japanese, referee, Yoshimi, Yamashita, set, m...   \n",
       "4      [Japanese, referee, Yoshimi, Yamashita, set, m...   \n",
       "...                                                  ...   \n",
       "52922  [@, RheaRipley_WWE, First, Aussie, win, #, Roy...   \n",
       "52923  [Check, Christian, Pulisic, Fifa, World, Cup, ...   \n",
       "52924  [Check, Gabriel, Jesus, Brazil, 2021-22, Panin...   \n",
       "52925  [Check, Erling, Haaland, Panini, Donruss, 2022...   \n",
       "52926  [@, scparametro, Welcome, fifa, world, cup, u2...   \n",
       "\n",
       "                                       lemmatizedContent  \n",
       "0      [FIFA, World, Cup, Trophy, arrives, Tehran, dl...  \n",
       "1      [Iran, put, FIFA, World, Cup, trophy, display,...  \n",
       "2      [WATCH, :, Japanese, referee, Yoshimi, Yamashi...  \n",
       "3      [Japanese, referee, Yoshimi, Yamashita, set, m...  \n",
       "4      [Japanese, referee, Yoshimi, Yamashita, set, m...  \n",
       "...                                                  ...  \n",
       "52922  [@, RheaRipley_WWE, First, Aussie, win, #, Roy...  \n",
       "52923  [Check, Christian, Pulisic, Fifa, World, Cup, ...  \n",
       "52924  [Check, Gabriel, Jesus, Brazil, 2021-22, Panin...  \n",
       "52925  [Check, Erling, Haaland, Panini, Donruss, 2022...  \n",
       "52926  [@, scparametro, Welcome, fifa, world, cup, u2...  \n",
       "\n",
       "[52927 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza zebranych opinii\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza za pomocą narzędzia NLTK Vader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "tweets[\"vader_compound\"] = tweets.apply(\n",
    "    lambda x: sentiment_analyzer.polarity_scores(\" \".join(x[\"lemmatizedContent\"]))[\"compound\"], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podsumowanie\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografia\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
